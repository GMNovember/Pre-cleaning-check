{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chardet in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (5.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langdetect in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from langdetect) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deep_translator in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (1.11.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from deep_translator) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from deep_translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install chardet\n",
    "!pip install langdetect\n",
    "!pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    \"\"\"Detect the encoding of a file.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(100000))  # Read a sample\n",
    "    return result['encoding']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(json_file, csv_file):\n",
    "    \"\"\"Convert JSON to CSV.\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.json_normalize(data)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load CSV or TSV data into a pandas DataFrame.\"\"\"\n",
    "    encoding = detect_encoding(file_path)\n",
    "    sep = '\\t' if file_path.endswith('.tsv') else ','\n",
    "    return pd.read_csv(file_path, encoding=encoding, sep=sep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_headers(df):\n",
    "    \"\"\"Check if headers exist and identify missing or duplicate headers.\"\"\"\n",
    "    print(f\"Checking headers\")\n",
    "    if df.columns.str.contains('Unnamed').any():\n",
    "        print(\"Warning: Dataset might be missing headers.\")\n",
    "    duplicate_headers = df.columns[df.columns.duplicated()].tolist()\n",
    "    if duplicate_headers:\n",
    "        print(f\"Duplicate headers found: {duplicate_headers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate foreign column headers to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_headers(df):\n",
    "    \"\"\"Detect and translate foreign language headers to English.\"\"\"\n",
    "    print(f\"Translating any foreign language headers\")\n",
    "    translated_headers = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            lang = detect(col)\n",
    "            if lang != 'en':\n",
    "                translated_col = GoogleTranslator(source='auto', target='en').translate(col)\n",
    "                translated_headers.append(translated_col)\n",
    "            else:\n",
    "                translated_headers.append(col)\n",
    "        except:\n",
    "            translated_headers.append(col)\n",
    "    df.columns = translated_headers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_structure(df):\n",
    "    \"\"\"Analyze dataset structure.\"\"\"\n",
    "    print(f\"Analyzing structure\")\n",
    "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "    print(\"Column data types:\")\n",
    "    print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find common columns across CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_columns(dfs):\n",
    "    \"\"\"Identify common columns across multiple CSV files.\"\"\"\n",
    "    print(f\"Searching for common columns across CSVs\")\n",
    "    common_cols = set(dfs[0].columns)\n",
    "    for df in dfs[1:]:\n",
    "        common_cols.intersection_update(df.columns)\n",
    "    print(f\"Common columns across datasets: {common_cols}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    \"\"\"Identify missing values.\"\"\"\n",
    "    print(f\"Identifying missing values\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing values per column:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_rows(df):\n",
    "    \"\"\"Identify duplicate rows.\"\"\"\n",
    "    print(f\"Identifying duplicate rows\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for invalid characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invalid_characters(df):\n",
    "    \"\"\"Identify invalid characters in text columns.\"\"\"\n",
    "    print(f\"Identifying invalid characters in text columns\")\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for mixed data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mixed_data_types(df):\n",
    "    \"\"\"Identify columns with mixed data types.\"\"\"\n",
    "    print(f\"Identifying columns with mixed data types\")\n",
    "    for col in df.columns:\n",
    "        types = df[col].map(type).nunique()\n",
    "        if types > 1:\n",
    "            print(f\"Column '{col}' has mixed data types.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(df):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    print(f\"Detecting outliers\")\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if not outliers.empty:\n",
    "            print(f\"Column '{col}' has {len(outliers)} potential outliers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\max_m\\AppData\\Local\\Temp\\ipykernel_14152\\1996712273.py:5: DtypeWarning: Columns (0,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path, encoding=encoding, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing zhaopin.csv...\n",
      "Checking headers\n",
      "Translating any foreign language headers\n",
      "Analyzing structure\n",
      "Rows: 1606289, Columns: 13\n",
      "Column data types:\n",
      "BID              object\n",
      "EMail            object\n",
      "Name             object\n",
      "Birthday         object\n",
      "ID card          object\n",
      "cell phone       object\n",
      "Education        object\n",
      "income           object\n",
      "Industry Code    object\n",
      "Working hours    object\n",
      "Field 15         object\n",
      "hangye           object\n",
      "memo             object\n",
      "dtype: object\n",
      "Identifying missing values\n",
      "Missing values per column:\n",
      "EMail                237\n",
      "Name               22707\n",
      "Birthday           21508\n",
      "ID card           199640\n",
      "cell phone         70736\n",
      "Education         239897\n",
      "income            707454\n",
      "Industry Code     707385\n",
      "Working hours     196537\n",
      "Field 15             218\n",
      "hangye           1371663\n",
      "memo              617709\n",
      "dtype: int64\n",
      "Identifying duplicate rows\n",
      "Duplicate rows: 199\n",
      "Identifying columns with mixed data types\n",
      "Column 'BID' has mixed data types.\n",
      "Column 'EMail' has mixed data types.\n",
      "Column 'Name' has mixed data types.\n",
      "Column 'Birthday' has mixed data types.\n",
      "Column 'ID card' has mixed data types.\n",
      "Column 'cell phone' has mixed data types.\n",
      "Column 'Education' has mixed data types.\n",
      "Column 'income' has mixed data types.\n",
      "Column 'Industry Code' has mixed data types.\n",
      "Column 'Working hours' has mixed data types.\n",
      "Column 'Field 15' has mixed data types.\n",
      "Column 'hangye' has mixed data types.\n",
      "Column 'memo' has mixed data types.\n",
      "Detecting outliers\n"
     ]
    }
   ],
   "source": [
    "def main(directory):\n",
    "    \"\"\"Main function to execute dataset analysis.\"\"\"\n",
    "    files = [f for f in os.listdir(directory) if f.endswith(('.csv', '.tsv', '.json'))]\n",
    "    datasets = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        if file.endswith('.json'):\n",
    "            csv_file = file_path.replace('.json', '.csv')\n",
    "            convert_json_to_csv(file_path, csv_file)\n",
    "            file_path = csv_file\n",
    "        df = load_data(file_path)\n",
    "        print(f\"Analyzing {file}...\")\n",
    "        check_headers(df)\n",
    "        translate_headers(df)\n",
    "        analyze_structure(df)\n",
    "        check_missing_values(df)\n",
    "        check_duplicate_rows(df)\n",
    "        check_mixed_data_types(df)\n",
    "        check_outliers(df)\n",
    "        datasets.append(df)\n",
    "    \n",
    "    if len(datasets) > 1:\n",
    "        find_common_columns(datasets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"./Zhaopin/datasets/records\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
