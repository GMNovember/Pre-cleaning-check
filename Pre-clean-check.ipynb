{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"Detect the encoding of a file.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(100000))  # Read a sample\n",
    "    return result['encoding']\n",
    "\n",
    "def convert_json_to_csv(json_file, csv_file):\n",
    "    \"\"\"Convert JSON to CSV.\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.json_normalize(data)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load CSV or TSV data into a pandas DataFrame.\"\"\"\n",
    "    encoding = detect_encoding(file_path)\n",
    "    sep = '\\t' if file_path.endswith('.tsv') else ','\n",
    "    return pd.read_csv(file_path, encoding=encoding, sep=sep)\n",
    "\n",
    "def check_headers(df):\n",
    "    \"\"\"Check if headers exist and identify missing or duplicate headers.\"\"\"\n",
    "    if df.columns.str.contains('Unnamed').any():\n",
    "        print(\"Warning: Dataset might be missing headers.\")\n",
    "    duplicate_headers = df.columns[df.columns.duplicated()].tolist()\n",
    "    if duplicate_headers:\n",
    "        print(f\"Duplicate headers found: {duplicate_headers}\")\n",
    "\n",
    "def translate_headers(df):\n",
    "    \"\"\"Detect and translate foreign language headers to English.\"\"\"\n",
    "    translated_headers = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            lang = detect(col)\n",
    "            if lang != 'en':\n",
    "                translated_col = GoogleTranslator(source='auto', target='en').translate(col)\n",
    "                translated_headers.append(translated_col)\n",
    "            else:\n",
    "                translated_headers.append(col)\n",
    "        except:\n",
    "            translated_headers.append(col)\n",
    "    df.columns = translated_headers\n",
    "\n",
    "def analyze_structure(df):\n",
    "    \"\"\"Analyze dataset structure.\"\"\"\n",
    "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "    print(\"Column data types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "def find_common_columns(dfs):\n",
    "    \"\"\"Identify common columns across multiple CSV files.\"\"\"\n",
    "    common_cols = set(dfs[0].columns)\n",
    "    for df in dfs[1:]:\n",
    "        common_cols.intersection_update(df.columns)\n",
    "    print(f\"Common columns across datasets: {common_cols}\")\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"Identify missing values.\"\"\"\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing values per column:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "def check_duplicate_rows(df):\n",
    "    \"\"\"Identify duplicate rows.\"\"\"\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "def check_invalid_characters(df):\n",
    "    \"\"\"Identify invalid characters in text columns.\"\"\"\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "def check_mixed_data_types(df):\n",
    "    \"\"\"Identify columns with mixed data types.\"\"\"\n",
    "    for col in df.columns:\n",
    "        types = df[col].map(type).nunique()\n",
    "        if types > 1:\n",
    "            print(f\"Column '{col}' has mixed data types.\")\n",
    "\n",
    "def check_outliers(df):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if not outliers.empty:\n",
    "            print(f\"Column '{col}' has {len(outliers)} potential outliers.\")\n",
    "\n",
    "def main(directory):\n",
    "    \"\"\"Main function to execute dataset analysis.\"\"\"\n",
    "    files = [f for f in os.listdir(directory) if f.endswith(('.csv', '.tsv', '.json'))]\n",
    "    datasets = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        if file.endswith('.json'):\n",
    "            csv_file = file_path.replace('.json', '.csv')\n",
    "            convert_json_to_csv(file_path, csv_file)\n",
    "            file_path = csv_file\n",
    "        df = load_data(file_path)\n",
    "        print(f\"Analyzing {file}...\")\n",
    "        check_headers(df)\n",
    "        translate_headers(df)\n",
    "        analyze_structure(df)\n",
    "        check_missing_values(df)\n",
    "        check_duplicate_rows(df)\n",
    "        check_mixed_data_types(df)\n",
    "        check_outliers(df)\n",
    "        datasets.append(df)\n",
    "    \n",
    "    if len(datasets) > 1:\n",
    "        find_common_columns(datasets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"./datasets\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
