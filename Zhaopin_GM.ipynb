{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES\n",
    "- pandas: Used for data manipulation and analysis, particularly with tabular data like CSV files.\n",
    "- numpy: Provides tools for numerical computations and handling multi-dimensional arrays efficiently.\n",
    "- logging: Enables the recording of log messages for debugging, tracking, and monitoring program execution.\n",
    "- re: Facilitates pattern matching and manipulation using regular expressions in strings.\n",
    "- os: Provides functions to interact with the operating system, including file and directory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as ny\n",
    "import logging\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: DEFINE FILE PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define File paths\n",
    "input_file = \"./datasets/records/zhaopin.csv\"\n",
    "intermediate_file_1 = \"./datasets/processed/step1/intermediate_1.csv\"\n",
    "intermediate_file_2 = \"./datasets/processed/step2/intermediate_2.csv\"\n",
    "intermediate_file_3 = \"./datasets/processed/step3/intermediate_3.csv\"\n",
    "intermediate_file_4 = \"./datasets/processed/step4/intermediate_4.csv\"\n",
    "intermediate_file_5 = \"./datasets/processed/step5/intermediate_5.csv\"\n",
    "intermediate_file_6 = \"./datasets/processed/step6/intermediate_6.csv\"\n",
    "intermediate_file_7 = \"./datasets/processed/step7/intermediate_7.csv\"\n",
    "intermediate_file_8 = \"./datasets/processed/step8/intermediate_8.csv\"\n",
    "intermediate_file_9 = \"./datasets/processed/step9/intermediate_9.csv\"\n",
    "intermediate_file_10 = \"./datasets/processed/step10/intermediate_10.csv\"\n",
    "garbage_file = \"./datasets/garbage/final_garbage_file.csv\"\n",
    "cleaned_file = \"./datasets/to-be-ingested/zhaopin-cleaned_output.csv\"\n",
    "log_file = \"./datasets/logs/zhaopin_cleaner_script.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: SET UP LOGGING\n",
    "- Logging both to the screen and toa log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up logging\n",
    "# Create a logger\n",
    "logger = logging.getLogger(\"script_logger\")\n",
    "logger.setLevel(logging.INFO)  # Set the minimum logging level\n",
    "\n",
    "# Create a file handler\n",
    "file_handler = logging.FileHandler(log_file, mode=\"a\")  # Append to 'script.log'\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Define a common format for both handlers\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add both handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: LOAD INITIAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\max_m\\AppData\\Local\\Temp\\ipykernel_28456\\2747940852.py:2: DtypeWarning: Columns (0,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load initial data\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: GET STATISTICS\n",
    "- Checking for raw data lngth and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data length: 1606289\n",
      "Missing values per column:\n",
      "BID             0\n",
      "EMail         237\n",
      "姓名          22707\n",
      "生日          21508\n",
      "身份证        199640\n",
      "手机          70736\n",
      "学历         239897\n",
      "收入         707454\n",
      "行业代码       707385\n",
      "工作时间       196537\n",
      "字段15          218\n",
      "hangye    1371663\n",
      "memo       617709\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify the raw data length (before cleaning)\n",
    "raw_data_length = len(df)\n",
    "print(f\"Raw data length: {raw_data_length}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean of the column\n",
    "#df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: PANDAS ACCEPTANCE/NORMALIZE DATASET\n",
    "- 5a - remove blank rows\n",
    "- 5b - remove blank columns\n",
    "- 5c - Remove non-ascii/special characters\n",
    "- 5d - Encode in UTF-8 format\n",
    "- 5e - Standardize text/text columns\n",
    "- 5f - Output to intermediate_file_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing rows with all NaN values\n",
      "Removing rows where all columns are empty or null\n",
      "Removubg non-ASCII characters/special characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\max_m\\AppData\\Local\\Temp\\ipykernel_28456\\1325827101.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(remove_non_ascii)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing text/text columns\n",
      "Cleaned data saved to ./datasets/processed/step5/intermediate_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Normalize Dataset\n",
    "\n",
    "def clean_dataframe(df, output_file):\n",
    "    # 5a: Removing rows with all NaN values\n",
    "    print(\"Removing rows with all NaN values\")\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    # 5b: Removing rows where all columns are empty or null\n",
    "    print(\"Removing rows where all columns are empty or null\")\n",
    "    problematic_rows = df[df.isnull().all(axis=1)].index\n",
    "    if not problematic_rows.empty:\n",
    "        df.drop(index=problematic_rows, inplace=True)\n",
    "\n",
    "    # 5c: Removing non-ASCII characters/special characters\n",
    "    print(\"Removubg non-ASCII characters/special characters\")\n",
    "    def remove_non_ascii(text):\n",
    "        if isinstance(text, str):\n",
    "            return re.sub(r'[\\x80-\\xFF]', '', text)  # Removing non-ASCII characters\n",
    "        return text\n",
    "\n",
    "    df = df.applymap(remove_non_ascii)\n",
    "\n",
    "    # 5d: Encode the data using UTF-8 format\n",
    "    # Pandas automatically handles encoding when writing CSV files, so no explicit action needed here.\n",
    "\n",
    "    # 5e: Standardizing text/text columns\n",
    "    print(\"Standardizing text/text columns\")\n",
    "    def standardize_text(text):\n",
    "        if isinstance(text, str):\n",
    "            # Lowercase text\n",
    "            text = text.lower()\n",
    "            # strip whitespace\n",
    "            text = text.strip()\n",
    "            # Remove non-ASCII characters (but preserve for foreign languages if necessary)\n",
    "            return re.sub(r'[\\x80-\\xFF]', '', text)\n",
    "        return text\n",
    "\n",
    "    text_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].apply(standardize_text)\n",
    "\n",
    "    # 5d: Output the data frame to a CSV file\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "clean_dataframe(df, intermediate_file_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: DROPPING AND RENAMING COLUMNS\n",
    "- 6a: Load data\n",
    "- 6b: Drop columns\n",
    "- 6c: Rename columns\n",
    "- 6d: Output to intermediate_file_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\max_m\\AppData\\Local\\Temp\\ipykernel_28456\\2913616572.py:4: DtypeWarning: Columns (0,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(intermediate_file_5)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Dropping and renaming columns\n",
    "\n",
    "# 6a: Load data\n",
    "df = pd.read_csv(intermediate_file_5)\n",
    "\n",
    "# 6b: Dropping Columns\n",
    "df = df.iloc[:,[1,2,3,4,5]]\n",
    "\n",
    "# 6c: Renaming Columns        \n",
    "# Rename a column (e.g., rename 'Name' to 'last_name')\n",
    "df.columns = ['email', 'name', 'date_of_birth', 'id_number', 'phone_number']\n",
    "\n",
    "# 6d: Output to intermediate_file_6\n",
    "df.to_csv(intermediate_file_6, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: DEALING WITH INVALID DATA\n",
    "- 7a: Load data\n",
    "- 7b: Validate and correct emails\n",
    "- 7c: Output to intermediate_file_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 14:02:12,928 - INFO - Validating and correcting email addresses\n",
      "2025-02-13 14:02:12,928 - INFO - Validating and correcting email addresses\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Dealing with invalid data\n",
    "# 7a: Load data\n",
    "def correct_email_domain(email):\n",
    "    \"\"\"\n",
    "    Corrects common typos in email domains.\n",
    "    \"\"\"\n",
    "    if not isinstance(email, str):\n",
    "        return email  # Return the input as-is if not a string\n",
    "    email = email.lower()\n",
    "    domain_corrections = {\n",
    "        \"hotmial.com\": \"hotmail.com\",\n",
    "        \"hotmil.com\": \"hotmail.com\",\n",
    "        \"hotnail.com\": \"hotmail.com\",\n",
    "        \"hotmiall.com\": \"hotmail.com\",\n",
    "        \"hotmial.ocm\": \"hotmail.com\",\n",
    "        \"hocmail.com\": \"hotmail.com\",\n",
    "        \"gnail.com\": \"gmail.com\",\n",
    "        \"yaho.com\": \"yahoo.com\",\n",
    "        \"outllok.com\": \"outlook.com\",\n",
    "    }\n",
    "    for typo, correct in domain_corrections.items():\n",
    "        if typo in email:\n",
    "            return email.replace(typo, correct)\n",
    "    return email\n",
    "\n",
    "# Validate Email Function\n",
    "def validate_email(email):\n",
    "    \"\"\"\n",
    "    Validates email addresses after correcting typos.\n",
    "    \"\"\"\n",
    "    if not isinstance(email, str):  # Ensure email is a string\n",
    "        return False, email  # Invalid if not a string\n",
    "    email = correct_email_domain(email)\n",
    "    # Corrected regex pattern\n",
    "    pattern = r\"^[a-zA-Z0-9!#$%&'*+/=?^_{|}~.-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "    return bool(re.match(pattern, email)), email\n",
    "\n",
    "# Main Validation and Correction Function\n",
    "def validate_and_correct_emails(input_file, output_file, garbage_file):\n",
    "    \"\"\"\n",
    "    Validates and corrects emails, saving valid and invalid data separately.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file, encoding=\"utf-8-sig\")\n",
    "    garbage = pd.DataFrame()\n",
    "\n",
    "    # Apply email validation and correction\n",
    "    df[\"is_valid_email\"], df[\"email\"] = zip(*df[\"email\"].apply(lambda x: validate_email(x)))\n",
    "\n",
    "    # Handle invalid emails\n",
    "    invalid_emails = df[~df[\"is_valid_email\"]].copy()\n",
    "    invalid_emails[\"Issue\"] = \"Invalid email\"\n",
    "\n",
    "    # Save invalid emails to the garbage file\n",
    "    garbage = pd.concat([garbage, invalid_emails], ignore_index=True)\n",
    "    garbage.to_csv(garbage_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Keep only valid emails in the original DataFrame\n",
    "    df = df[df[\"is_valid_email\"]].drop(columns=[\"is_valid_email\"])\n",
    "    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "logger.info(\"Validating and correcting email addresses\")\n",
    "validate_and_correct_emails(intermediate_file_6, intermediate_file_7, garbage_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8: DEALING WITH INVALID DATA\n",
    "- 8a: Load data\n",
    "- 8b: Validate and correct mobile_phone\n",
    "- 8c: Output to intermediate_file_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 14:04:01,425 - INFO - Cleaning phone numbers\n",
      "2025-02-13 14:04:01,425 - INFO - Cleaning phone numbers\n"
     ]
    }
   ],
   "source": [
    "# Step 8:\n",
    "# 8a: Load data\n",
    "df = pd.read_csv(intermediate_file_7)\n",
    "\n",
    "# 8b: Clean Phone Numbers\n",
    "def clean_phone_number(phone_number):\n",
    "    \"\"\"\n",
    "    Cleans the phone_number field:\n",
    "    1. Leaves empty phone_number fields unchanged.\n",
    "    2. Strips non-numeric characters (e.g., letters, +, -, (, ), *, spaces, etc.).\n",
    "    3. Ensures valid phone numbers have lengths between 7 and 15 digits.\n",
    "       Invalid numbers are returned as blank.\n",
    "    \"\"\"\n",
    "    if pd.isnull(phone_number) or phone_number == \"\":\n",
    "        # Leave empty fields unchanged\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to string for processing\n",
    "    phone_number = str(phone_number)\n",
    "\n",
    "    # Strip all non-numeric characters\n",
    "    cleaned_number = re.sub(r\"\\D\", \"\", phone_number)  # Retains only digits (0-9)\n",
    "\n",
    "    # Validate phone numbers length (7-15 characters)\n",
    "    if 7 <= len(cleaned_number) <= 15:\n",
    "        return cleaned_number\n",
    "    else:\n",
    "        # Return blank for invalid phone numbers\n",
    "        return \"\"\n",
    "\n",
    "logger.info(\"Cleaning phone numbers\")\n",
    "\n",
    "# Ensure the column exists before cleaning\n",
    "if \"phone_number\" in df.columns:\n",
    "    # Apply the cleaning function to the 'phone_number' column\n",
    "    df[\"phone_number\"] = df[\"phone_number\"].apply(clean_phone_number)\n",
    "else:\n",
    "    logger.warning(\"'phone_number' column is missing in the input file.\")\n",
    "\n",
    "# 8c: Output to intermediate_file_8\n",
    "df.to_csv(intermediate_file_8, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 9: DROPPING INSUFFICIENT DATA ROWS\n",
    "- 9a: Load data\n",
    "- 9b: Drop rows with insufficient data\n",
    "- 9c: Output to intermediate_file_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Dealing with Invalid Data\n",
    "\n",
    "# 9a: Load data\n",
    "df = pd.read_csv(intermediate_file_8)\n",
    "\n",
    "# 9b: Remove rows where 'email', 'name' or 'id_number' are blank\n",
    "df_cleaned = df.replace('', None).dropna(subset=['email', 'name', 'id_number'])\n",
    "\n",
    "# 9c: Output to intermediate_file_9\n",
    "df_cleaned.to_csv(intermediate_file_9, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 10: REMOVE DUPLICATE RECORDS\n",
    "- 10a: Load data\n",
    "- 10b: Drop duplicate records\n",
    "- 10c: Put phone number back to int64\n",
    "- 10d: Output to final cleaned data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 14:07:02,410 - INFO - Checking for duplicate email addresses\n",
      "2025-02-13 14:07:02,410 - INFO - Checking for duplicate email addresses\n",
      "C:\\Users\\max_m\\AppData\\Local\\Temp\\ipykernel_28456\\2505399779.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  duplicates[\"Issue\"] = \"duplicate email\"\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Remove duplicate files\n",
    "\n",
    "# 10a: Load data\n",
    "df = pd.read_csv(intermediate_file_9)\n",
    "garbage = pd.read_csv(garbage_file)\n",
    "\n",
    "# 10b: Remove duplicate rows\n",
    "df = df.replace('', None).dropna(subset=['email', 'name', 'id_number'])\n",
    "\n",
    "# 10b: Check for duplicate emails\n",
    "logger.info(\"Checking for duplicate email addresses\")\n",
    "duplicates = df[df.duplicated(subset=[\"email\"], keep=\"first\")]\n",
    "duplicates[\"Issue\"] = \"duplicate email\"\n",
    "garbage = pd.concat([garbage, duplicates], ignore_index=True)\n",
    "    \n",
    "# Remove duplicates from the cleaned data\n",
    "df = df.drop_duplicates(subset=[\"email\"], keep=\"first\")\n",
    "\n",
    "# 10c: Ensure phone_number is treated as a string to avoid decimals in the CSV\n",
    "if 'phone_number' in df.columns:\n",
    "    df['phone_number'] = df['phone_number'].astype('Int64')\n",
    "\n",
    "\n",
    "# 10d: Output to final cleaned file\n",
    "df.to_csv(cleaned_file, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 11: FINAL REPORT AND CLEANUP\n",
    "- 11a: Print statistics\n",
    "- 11b: Delete intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 14:07:31,686 - WARNING - Intermediate file not found: ./datasets/processed/step1/intermediate_1.csv\n",
      "2025-02-13 14:07:31,686 - WARNING - Intermediate file not found: ./datasets/processed/step1/intermediate_1.csv\n",
      "2025-02-13 14:07:31,691 - WARNING - Intermediate file not found: ./datasets/processed/step2/intermediate_2.csv\n",
      "2025-02-13 14:07:31,691 - WARNING - Intermediate file not found: ./datasets/processed/step2/intermediate_2.csv\n",
      "2025-02-13 14:07:31,694 - WARNING - Intermediate file not found: ./datasets/processed/step3/intermediate_3.csv\n",
      "2025-02-13 14:07:31,694 - WARNING - Intermediate file not found: ./datasets/processed/step3/intermediate_3.csv\n",
      "2025-02-13 14:07:31,697 - WARNING - Intermediate file not found: ./datasets/processed/step4/intermediate_4.csv\n",
      "2025-02-13 14:07:31,697 - WARNING - Intermediate file not found: ./datasets/processed/step4/intermediate_4.csv\n",
      "2025-02-13 14:07:31,800 - INFO - Deleted intermediate file: ./datasets/processed/step5/intermediate_5.csv\n",
      "2025-02-13 14:07:31,800 - INFO - Deleted intermediate file: ./datasets/processed/step5/intermediate_5.csv\n",
      "2025-02-13 14:07:31,854 - INFO - Deleted intermediate file: ./datasets/processed/step6/intermediate_6.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Data:\n",
      "                        email name date_of_birth           id_number  \\\n",
      "0       shipeijie_001@163.com  史培杰    1984-03-01  370902198403264812   \n",
      "1   liuchunhua770512@yahoo.cn  刘春华    1977-05-01  65010319770512604x   \n",
      "2  zhaojing830803@hotmail.com   赵静    1983-08-01  421002198308031881   \n",
      "3      shiming586@hotmail.com  夏世明    1979-02-01  362222197902253512   \n",
      "4             bossjj2@163.com  朱泽宇    1986-06-05  320683198606014314   \n",
      "\n",
      "   phone_number  \n",
      "0  113774254415  \n",
      "1  113482701254  \n",
      "2  113764095160  \n",
      "3  113381756335  \n",
      "4  115301612043  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 14:07:31,854 - INFO - Deleted intermediate file: ./datasets/processed/step6/intermediate_6.csv\n",
      "2025-02-13 14:07:31,901 - INFO - Deleted intermediate file: ./datasets/processed/step7/intermediate_7.csv\n",
      "2025-02-13 14:07:31,901 - INFO - Deleted intermediate file: ./datasets/processed/step7/intermediate_7.csv\n",
      "2025-02-13 14:07:31,953 - INFO - Deleted intermediate file: ./datasets/processed/step8/intermediate_8.csv\n",
      "2025-02-13 14:07:31,953 - INFO - Deleted intermediate file: ./datasets/processed/step8/intermediate_8.csv\n",
      "2025-02-13 14:07:32,001 - INFO - Deleted intermediate file: ./datasets/processed/step9/intermediate_9.csv\n",
      "2025-02-13 14:07:32,001 - INFO - Deleted intermediate file: ./datasets/processed/step9/intermediate_9.csv\n",
      "2025-02-13 14:07:32,006 - WARNING - Intermediate file not found: ./datasets/processed/step10/intermediate_10.csv\n",
      "2025-02-13 14:07:32,006 - WARNING - Intermediate file not found: ./datasets/processed/step10/intermediate_10.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Final report and cleanup\n",
    "\n",
    "# 11a: Final cleaned data preview\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# 11b: Delete intermediate files\n",
    "# List of intermediate files to be cleaned up\n",
    "intermediate_files = [\n",
    "    intermediate_file_1,\n",
    "    intermediate_file_2,\n",
    "    intermediate_file_3,\n",
    "    intermediate_file_4,\n",
    "    intermediate_file_5,\n",
    "    intermediate_file_6,\n",
    "    intermediate_file_7,\n",
    "    intermediate_file_8,\n",
    "    intermediate_file_9,\n",
    "    intermediate_file_10\n",
    "]\n",
    "\n",
    "for file in intermediate_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        logger.info(f\"Deleted intermediate file: {file}\")\n",
    "    else:\n",
    "        logger.warning(f\"Intermediate file not found: {file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
