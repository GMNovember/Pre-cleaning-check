{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Libraries Importation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datetime in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (5.5)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from datetime) (7.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from datetime) (2025.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\max_m\\appdata\\roaming\\python\\python313\\site-packages (from zope.interface->datetime) (75.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import unicodedata\n",
    "import datetime as dt\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Creation\n",
    "Processed_path = f\"./zhaopin/datasets/processed/\" # Processed Data File(s) Exist Here \n",
    "records_path = f\"./zhaopin/datasets/records/\" # Raw Data File Exists Here\n",
    "garbage_path =f\"./zhaopin/datasets/garbage/\" # Garbage File Exists Here\n",
    "final_path = f\"./zhaopin/datasets/to-be-ingested/\" # Final Clean Data File Exists Here\n",
    "\n",
    "# Create the folder\n",
    "# os.makedirs(Processed_path, exist_ok=True)\n",
    "# os.makedirs(records_path, exist_ok=True)\n",
    "# os.makedirs(garbage_path, exist_ok=True)\n",
    "# os.makedirs(final_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logging configuration\n",
    "log_file_path = f'./zhaopin/datasets/logs/zhaopin_processing.log'\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)  # Ensure the log directory exists\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(log_file_path, mode='w', encoding='utf-8')]\n",
    ")\n",
    "\n",
    "logging.info(\"Logging has been set up. All logs will be saved to the file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis & Stats (For Post Validation Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Analysis: total files to process: 1\n",
      "[1/1] Processing file: zhaopin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\max_m\\AppData\\Local\\Temp\\ipykernel_25676\\1431167071.py:40: DtypeWarning: Columns (0,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk-level statistics saved to: ./zhaopin/datasets/logs/reports\\chunk_statistics.csv\n",
      "Combined chunk statistics saved to: ./zhaopin/datasets/logs/reports\\overall_totals.csv\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_chunks(folder_path, report_file, threshold=3, start_file=0, num_files=None):\n",
    "    \"\"\"\n",
    "    Analyze dataset chunks for statistics such as total rows, rows with missing values, duplicates, and special characters.\n",
    "    Optimized for faster processing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the report folder exists\n",
    "    report_folder = os.path.dirname(report_file)\n",
    "    os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "    # Prepare to store chunk-level statistics\n",
    "    chunk_stats = []\n",
    "\n",
    "    # Prepare for overall totals\n",
    "    overall_totals = {\n",
    "        \"Total Rows\": 0,\n",
    "        \"Total Rows with Missing Columns\": 0,\n",
    "        \"Total Duplicate Rows\": 0,\n",
    "        \"Special Characters Found\": Counter()\n",
    "    }\n",
    "\n",
    "    # Get list of files and apply slicing based on start_file and num_files\n",
    "    all_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".csv\")])\n",
    "    selected_files = all_files[start_file:start_file + num_files] if num_files else all_files[start_file:]\n",
    "\n",
    "    print(f\"Dataset Analysis: total files to process: {len(selected_files)}\")\n",
    "    logging.info(f\"Dataset Analysis: total files to process: {len(selected_files)}\")\n",
    "\n",
    "    # Define regex for special characters once (with capture group)\n",
    "    special_char_pattern = r\"([^a-zA-Z0-9\\s])\"  # Add capture group\n",
    "\n",
    "    # Process each selected CSV file in the folder\n",
    "    for idx, filename in enumerate(selected_files, start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            print(f\"[{idx}/{len(selected_files)}] Processing file: {filename}\")\n",
    "            logging.info(f\"[{idx}/{len(selected_files)}] Processing file: {filename}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Analyze chunk statistics\n",
    "            total_rows = len(df)\n",
    "            missing_rows = df.isnull().sum(axis=1).ge(threshold).sum()\n",
    "            duplicate_rows = df.duplicated().sum()\n",
    "\n",
    "            # Identify special characters (vectorized operation)\n",
    "            special_chars_found = Counter()\n",
    "            for col in df.columns:\n",
    "                col_special_chars = df[col].astype(str).str.extractall(special_char_pattern).stack().value_counts()\n",
    "                special_chars_found.update(col_special_chars.to_dict())\n",
    "\n",
    "            # Append chunk statistics\n",
    "            chunk_stats.append({\n",
    "                \"File\": filename,\n",
    "                \"Total Rows\": total_rows,\n",
    "                \"Rows with Missing Columns (> threshold)\": missing_rows,\n",
    "                \"Duplicate Rows\": duplicate_rows,\n",
    "                \"Special Characters Found\": ', '.join(special_chars_found.keys()),\n",
    "                \"Special Character Count\": sum(special_chars_found.values())\n",
    "            })\n",
    "\n",
    "            # Update overall totals\n",
    "            overall_totals[\"Total Rows\"] += total_rows\n",
    "            overall_totals[\"Total Rows with Missing Columns\"] += missing_rows\n",
    "            overall_totals[\"Total Duplicate Rows\"] += duplicate_rows\n",
    "            overall_totals[\"Special Characters Found\"].update(special_chars_found)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Empty file skipped: {filename}\")\n",
    "            logging.error(f\"Empty file skipped: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            logging.error(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Save chunk-level statistics to CSV\n",
    "    chunk_stats_df = pd.DataFrame(chunk_stats)\n",
    "    chunk_stats_report = os.path.join(report_folder, \"chunk_statistics.csv\")\n",
    "    chunk_stats_df.to_csv(chunk_stats_report, index=False)\n",
    "    print(f\"Chunk-level statistics saved to: {chunk_stats_report}\")\n",
    "    logging.info(f\"Chunk-level statistics saved to: {chunk_stats_report}\")\n",
    "\n",
    "    # Save overall totals to CSV\n",
    "    overall_totals_report = os.path.join(report_folder, \"overall_totals.csv\")\n",
    "    overall_totals_df = pd.DataFrame([{\n",
    "        \"Total Rows\": overall_totals[\"Total Rows\"],\n",
    "        \"Rows with Missing Columns (> threshold)\": overall_totals[\"Total Rows with Missing Columns\"],\n",
    "        \"Duplicate Rows\": overall_totals[\"Total Duplicate Rows\"],\n",
    "        \"Special Characters Found\": ', '.join(overall_totals[\"Special Characters Found\"].keys()),\n",
    "        \"Special Character Count\": sum(overall_totals[\"Special Characters Found\"].values())\n",
    "    }])\n",
    "    overall_totals_df.to_csv(overall_totals_report, index=False)\n",
    "    print(f\"Combined chunk statistics saved to: {overall_totals_report}\")\n",
    "    logging.info(f\"Combined chunk statistics saved to: {overall_totals_report}\")\n",
    "\n",
    "folder_path = \"./zhaopin/datasets/records/\"\n",
    "report_file = \"./zhaopin/datasets/logs/reports/zhaopin_dataset_statistics.csv\"\n",
    "\n",
    "# Set the start file index and number of files to process\n",
    "start_file = 0  # Start from the 1st file (index 0)\n",
    "num_files = None   # Process 1 file\n",
    "\n",
    "# Analyze the dataset chunks\n",
    "analyze_dataset_chunks(folder_path, report_file, threshold=3, start_file=start_file, num_files=num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column by Column Analysis: total files to process: 1\n",
      "[1/1] Processing file: zhaopin-cleaned_output.csv\n",
      "Column statistics saved to: ./zhaopin/datasets/reports/zhaopin_column_statistics.csv\n",
      "Combined column statistics saved to: ./zhaopin/datasets/reports/zhaopin_column_statistics_combined_totals.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to create statistics for each column. \n",
    "# Two CSvs produced, one which shows stats per chunk, the other with stats for all chunks in folder.\n",
    "def analyze_csv_columns(folder_path, report_file, start_file=0, num_files=None):\n",
    "    \"\"\"\n",
    "    Analyze CSV files in a folder for column-level statistics and combined totals across all chunks.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        report_file (str): Path to save the column statistics report.\n",
    "        start_file (int): Index of the file to start processing from.\n",
    "        num_files (int or None): Number of files to process. If None, processes all files starting from start_file.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the report folder exists\n",
    "    report_folder = os.path.dirname(report_file)\n",
    "    os.makedirs(report_folder, exist_ok=True)\n",
    "    \n",
    "    # Prepare to store column statistics and combined totals\n",
    "    column_stats = []\n",
    "    combined_totals = defaultdict(lambda: {\n",
    "        \"Total Values\": 0,\n",
    "        \"Missing Values\": 0,\n",
    "        \"Duplicate Values\": 0,\n",
    "        \"Special Characters Found\": Counter(),\n",
    "        \"Special Character Count\": 0\n",
    "    })\n",
    "\n",
    "    # Helper function to find special characters\n",
    "    def find_special_characters(value):\n",
    "        special_chars = re.findall(r\"[^a-zA-Z0-9\\s]\", str(value))\n",
    "        return special_chars if special_chars else []\n",
    "\n",
    "    # Get list of files and apply slicing based on start_file and num_files\n",
    "    all_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".csv\")])\n",
    "    selected_files = all_files[start_file:start_file + num_files] if num_files else all_files[start_file:]\n",
    "\n",
    "    print(f\"Column by Column Analysis: total files to process: {len(selected_files)}\")\n",
    "    logging.info(f\"Column by Column Analysis: total files to process: {len(selected_files)}\")\n",
    "    \n",
    "    # Process each selected CSV file in the folder\n",
    "    for idx, filename in enumerate(selected_files, start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            print(f\"[{idx}/{len(selected_files)}] Processing file: {filename}\")\n",
    "            logging.info(f\"[{idx}/{len(selected_files)}] Processing file: {filename}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Analyze each column\n",
    "            for col in df.columns:\n",
    "                total_values = len(df[col])\n",
    "                missing_values = df[col].isnull().sum()\n",
    "                duplicate_values = df[col].duplicated().sum()\n",
    "                special_chars_found = Counter()\n",
    "                \n",
    "                # Find special characters in the column\n",
    "                for value in df[col]:\n",
    "                    special_chars = find_special_characters(value)\n",
    "                    special_chars_found.update(special_chars)\n",
    "                \n",
    "                # Append statistics for the column in this file\n",
    "                column_stats.append({\n",
    "                    \"File\": filename,\n",
    "                    \"Column\": col,\n",
    "                    \"Total Values\": total_values,\n",
    "                    \"Missing Values\": missing_values,\n",
    "                    \"Duplicate Values\": duplicate_values,\n",
    "                    \"Special Characters Found\": ', '.join(set(special_chars_found)),\n",
    "                    \"Special Character Count\": sum(special_chars_found.values())\n",
    "                })\n",
    "\n",
    "                # Update combined totals\n",
    "                combined_totals[col][\"Total Values\"] += total_values\n",
    "                combined_totals[col][\"Missing Values\"] += missing_values\n",
    "                combined_totals[col][\"Duplicate Values\"] += duplicate_values\n",
    "                combined_totals[col][\"Special Characters Found\"].update(special_chars_found)\n",
    "                combined_totals[col][\"Special Character Count\"] += sum(special_chars_found.values())\n",
    "        \n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Empty file skipped: {filename}\")\n",
    "            logging.warning(f\"Empty file skipped: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            logging.error(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Save column statistics for individual files to CSV\n",
    "    column_stats_df = pd.DataFrame(column_stats)\n",
    "    column_stats_report = report_file\n",
    "    column_stats_df.to_csv(column_stats_report, index=False)\n",
    "    print(f\"Column statistics saved to: {column_stats_report}\")\n",
    "    logging.info(f\"Column statistics saved to: {column_stats_report}\")\n",
    "\n",
    "    # Save combined totals to a separate CSV\n",
    "    combined_stats_data = []\n",
    "    for col, stats in combined_totals.items():\n",
    "        combined_stats_data.append({\n",
    "            \"Column\": col,\n",
    "            \"Total Values\": stats[\"Total Values\"],\n",
    "            \"Missing Values\": stats[\"Missing Values\"],\n",
    "            \"Duplicate Values\": stats[\"Duplicate Values\"],\n",
    "            \"Special Characters Found\": ', '.join(set(stats[\"Special Characters Found\"])),\n",
    "            \"Special Character Count\": stats[\"Special Character Count\"]\n",
    "        })\n",
    "\n",
    "    combined_stats_df = pd.DataFrame(combined_stats_data)\n",
    "    combined_stats_report = os.path.splitext(report_file)[0] + \"_combined_totals.csv\"\n",
    "    combined_stats_df.to_csv(combined_stats_report, index=False)\n",
    "    print(f\"Combined column statistics saved to: {combined_stats_report}\")\n",
    "    logging.info(f\"Combined column statistics saved to: {combined_stats_report}\")\n",
    "\n",
    "# Example Usage\n",
    "folder_path = \"./zhaopin/datasets/to-be-ingested/\"\n",
    "report_file = \"./zhaopin/datasets/reports/zhaopin_column_statistics.csv\"\n",
    "\n",
    "# Set the start file index and number of files to process\n",
    "start_file = 0  # Start from the first file\n",
    "num_files = None  # Process all files\n",
    "\n",
    "analyze_csv_columns(folder_path, report_file, start_file=start_file, num_files=num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Analysis: Processing 1 files. Analyzing columns: ['name', 'email'].\n",
      "[1/1] Processing file: zhaopin-cleaned_output.csv\n",
      "Column statistics saved to: ./zhaopin/datasets/logs/reports/zhaopin_custom_column_stats.csv\n",
      "Combined column statistics saved to: ./zhaopin/datasets/logs/reports/zhaopin_custom_column_stats_combined.csv\n"
     ]
    }
   ],
   "source": [
    "#Analyze CSV files in a folder based on specified columns and functions.\n",
    "#Adds a special characters summary to the combined statistics file without increasing processing time.\n",
    "\n",
    "# Validation functions\n",
    "def is_alphabetical(value):\n",
    "    return str(value).isalpha()\n",
    "\n",
    "def is_alphanumeric(value):\n",
    "    return str(value).isalnum()\n",
    "\n",
    "def is_numeric(value):\n",
    "    return str(value).isdigit()\n",
    "\n",
    "def contains_special_chars(value):\n",
    "    return bool(re.search(r\"[^a-zA-Z0-9\\s]\", str(value)))\n",
    "\n",
    "def is_valid_date(value):\n",
    "    if pd.isnull(value) or value == \"\":\n",
    "        return False\n",
    "    if not re.match(r\"^\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}$\", str(value)):\n",
    "        return False\n",
    "    try:\n",
    "        pd.to_datetime(value, errors=\"raise\")\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def is_valid_email(value):\n",
    "    return bool(re.match(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\", str(value)))\n",
    "\n",
    "def is_missing(value):\n",
    "    return pd.isnull(value)\n",
    "\n",
    "def is_duplicate(column):\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    for value in column:\n",
    "        if value in seen:\n",
    "            duplicates.append(True)\n",
    "        else:\n",
    "            duplicates.append(False)\n",
    "            seen.add(value)\n",
    "    return pd.Series(duplicates, index=column.index)\n",
    "\n",
    "def list_special_characters(value):\n",
    "    special_chars = re.findall(r\"[^a-zA-Z0-9\\s]\", str(value))\n",
    "    return \", \".join(special_chars) if special_chars else None\n",
    "\n",
    "def extract_unique_values(list_of_strings):\n",
    "  \"\"\"Extracts unique values from a list of strings, splitting each string by commas.\n",
    "\n",
    "  Args:\n",
    "    list_of_strings: A list of strings.\n",
    "\n",
    "  Returns:\n",
    "    A set of unique values.\n",
    "  \"\"\"\n",
    "\n",
    "  all_values = []\n",
    "  for string in list_of_strings:\n",
    "    if string is not None:\n",
    "      values = string.split(', ')\n",
    "      all_values.extend(values)\n",
    "\n",
    "  unique_values = set(all_values)\n",
    "  return unique_values\n",
    "\n",
    "def analyze_csv_columns_with_functions(folder_path, report_file, columns, functions, start_file=0, num_files=None):\n",
    "    \"\"\"\n",
    "    Analyze CSV files in a folder based on specified columns and functions.\n",
    "    Adds a special characters summary to the combined statistics file without increasing processing time.\n",
    "    \"\"\"\n",
    "    # Ensure the report folder exists\n",
    "    report_folder = os.path.dirname(report_file)\n",
    "    os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "    # Prepare to store statistics\n",
    "    column_stats = []\n",
    "    combined_totals = defaultdict(lambda: defaultdict(int))\n",
    "    special_characters_per_file = []\n",
    "    aggregated_special_characters = defaultdict(set)  # To store cumulative special characters for each column\n",
    "\n",
    "    # Get list of files and apply slicing\n",
    "    all_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".csv\")])\n",
    "    selected_files = all_files[start_file:start_file + num_files] if num_files else all_files[start_file:]\n",
    "\n",
    "    print(f\"Custom Analysis: Processing {len(selected_files)} files. Analyzing columns: {columns}.\")\n",
    "    logging.info(f\"Custom Analysis: Processing {len(selected_files)} files. Analyzing columns: {columns}.\")\n",
    "\n",
    "    # Process each selected file\n",
    "    for idx, filename in enumerate(selected_files, start=1):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            print(f\"[{idx}/{len(selected_files)}] Processing file: {filename}\")\n",
    "            logging.info(f\"[{idx}/{len(selected_files)}] Processing file: {filename}\")\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Analyze specified columns\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    print(f\"Column {col} not found in {filename}. Skipping.\")\n",
    "                    logging.warning(f\"Column {col} not found in {filename}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                for func_name, func in functions.items():\n",
    "                    try:\n",
    "                        if func_name == \"Is Duplicate\":\n",
    "                            results = is_duplicate(df[col])\n",
    "                        elif func_name == \"List Special Characters\":\n",
    "                            # List special characters for this file and column\n",
    "                            results = df[col].apply(func)\n",
    "                            special_characters_summary = extract_unique_values(results)\n",
    "                            aggregated_special_characters[col].update(special_characters_summary)  # Aggregate\n",
    "                            special_characters_per_file.append({\n",
    "                                \"File\": filename,\n",
    "                                \"Column\": col,\n",
    "                                \"Special Characters Summary\": str(special_characters_summary)\n",
    "                            })\n",
    "                            column_stats.append({\n",
    "                                \"File\": filename,\n",
    "                                \"Column\": col,\n",
    "                                \"Function\": func_name,\n",
    "                                \"Special Characters Summary\": str(special_characters_summary)\n",
    "                            })\n",
    "                            continue\n",
    "                        else:\n",
    "                            results = df[col].apply(func)\n",
    "\n",
    "                        total_values = len(df[col])\n",
    "                        matches = results.sum() if func_name != \"Is Duplicate\" else results.sum()\n",
    "                        non_matches = total_values - matches\n",
    "\n",
    "                        # Append statistics for the column in this file\n",
    "                        column_stats.append({\n",
    "                            \"File\": filename,\n",
    "                            \"Column\": col,\n",
    "                            \"Function\": func_name,\n",
    "                            \"Matches\": matches,\n",
    "                            \"Non-Matches\": non_matches,\n",
    "                            \"Total Values\": total_values\n",
    "                        })\n",
    "\n",
    "                        # Update combined totals\n",
    "                        combined_totals[col][f\"{func_name}_Matches\"] += matches\n",
    "                        combined_totals[col][f\"{func_name}_Non-Matches\"] += non_matches\n",
    "                        combined_totals[col][\"Total Values\"] += total_values\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error applying function {func_name} on column {col} in {filename}: {e}\")\n",
    "                        logging.error(f\"Error applying function {func_name} on column {col} in {filename}: {e}\")\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Empty file skipped: {filename}\")\n",
    "            logging.warning(f\"Empty file skipped: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            logging.error(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Save column statistics for individual files to CSV\n",
    "    column_stats_df = pd.DataFrame(column_stats)\n",
    "    column_stats_df.to_csv(report_file, index=False)\n",
    "    print(f\"Column statistics saved to: {report_file}\")\n",
    "    logging.info(f\"Column statistics saved to: {report_file}\")\n",
    "\n",
    "    # Generate special characters summary for the combined statistics file\n",
    "    unique_special_characters = set()\n",
    "    for entry in special_characters_per_file:\n",
    "        special_chars = eval(entry[\"Special Characters Summary\"])  # Safely parse string dictionary\n",
    "        unique_special_characters.update(special_chars)\n",
    "\n",
    "    # Save combined totals and special characters summary to a separate CSV\n",
    "    combined_stats_data = []\n",
    "    for col, stats in combined_totals.items():\n",
    "        combined_stats_data.append({\n",
    "            \"Column\": col,\n",
    "            **stats,\n",
    "            \"Special Characters (Aggregated)\": \", \".join(sorted(aggregated_special_characters[col]))\n",
    "        })\n",
    "\n",
    "    # Append unique special characters summary for all columns\n",
    "    combined_stats_data.append({\n",
    "        \"Column\": \"All Columns\",\n",
    "        \"Unique Special Characters\": \", \".join(sorted(unique_special_characters)),\n",
    "        #\"Total Special Characters\": len(unique_special_characters)\n",
    "    })\n",
    "\n",
    "    combined_stats_df = pd.DataFrame(combined_stats_data)\n",
    "    combined_stats_report = os.path.splitext(report_file)[0] + \"_combined.csv\"\n",
    "    combined_stats_df.to_csv(combined_stats_report, index=False)\n",
    "    print(f\"Combined column statistics saved to: {combined_stats_report}\")\n",
    "    logging.info(f\"Combined column statistics saved to: {combined_stats_report}\")\n",
    "\n",
    "# Example Usage\n",
    "folder_path = \"./zhaopin/datasets/to-be-ingested/\"\n",
    "report_file = \"./zhaopin/datasets/logs/reports/zhaopin_custom_column_stats.csv\"\n",
    "\n",
    "columns_to_analyze = [\"name\",\"email\"]\n",
    "analysis_functions = {\n",
    "    \"Is Valid Email\": is_valid_email,\n",
    "    #\"Contains Special Characters\": contains_special_chars,\n",
    "    \"List Special Characters\": list_special_characters,\n",
    "    \"Is Missing\": is_missing,\n",
    "    \"Is Duplicate\": None,\n",
    "}\n",
    "\n",
    "start_file = 0  # Start from the first file\n",
    "num_files = None  # Process all files\n",
    "\n",
    "analyze_csv_columns_with_functions(folder_path, report_file, columns_to_analyze, analysis_functions, start_file, num_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
